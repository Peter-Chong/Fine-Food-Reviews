{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('../Reviews.csv')\n",
    "all_df = all_df[['Score', 'Text']]\n",
    "all_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = all_df[all_df.Score == 5]\n",
    "negative_reviews = all_df[all_df.Score != 5]\n",
    "\n",
    "positive_reviews = positive_reviews['Text'].astype(str).values.tolist()\n",
    "negative_reviews = negative_reviews['Text'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "pos_cutoff = int(len(positive_reviews)*0.8)\n",
    "neg_cutoff = int(len(negative_reviews)*0.8)\n",
    "\n",
    "test_pos = positive_reviews[pos_cutoff:]\n",
    "train_pos = positive_reviews[:pos_cutoff]\n",
    "test_neg = negative_reviews[neg_cutoff:]\n",
    "train_neg = negative_reviews[:neg_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis = 0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (314940, 1)\n",
      "test_y.shape = (78735, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing  \n",
    "### Stemming, remove stop words, tokenize reviews and build frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    \"\"\"Process review function.\n",
    "    Input:\n",
    "        review: a string containing a review\n",
    "    Output:\n",
    "        reviews_clean: a list of words containing the processed review\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove html tags like <br />\n",
    "    review = re.sub(r'<.*?>', ' ', review) # .* is for greedy and .*? makes it not greedy\n",
    "    # remove --- or --\n",
    "    review = re.sub(r'---', ' ', review)\n",
    "    review = re.sub(r'--', ' ', review)\n",
    "    # remove numbers\n",
    "    review = re.sub(r'[0-9]', '', review)\n",
    "    \n",
    "    # tokenize review\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    review_tokens = tokenizer.tokenize(review)\n",
    "\n",
    "    reviews_clean = []\n",
    "    for word in review_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            reviews_clean.append(stem_word)\n",
    "\n",
    "    return reviews_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(reviews, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        reviews: a list of reviews\n",
    "        ys: an m x 1 array with the sentiment label of each review\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    freqs = {}\n",
    "    for y, review in zip(yslist, reviews):\n",
    "        for word in process_review(review):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "            \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is one of the best salsas that I have found in a long time but stay away from the variety pack. The other two that come with it are not worth your money. \n",
      "\n",
      "['one', 'best', 'salsa', 'found', 'long', 'time', 'stay', 'away', 'varieti', 'pack', 'two', 'come', 'worth', 'money'] \n",
      "\n",
      "You can buy this at the pet store for about 2.19 a can . on the web here this is a rip. Sorry \n",
      "\n",
      "['buy', 'pet', 'store', 'web', 'rip', 'sorri'] \n",
      "\n",
      "frequencies:\n",
      "{('one', 1.0): 1, ('best', 1.0): 1, ('salsa', 1.0): 1, ('found', 1.0): 1, ('long', 1.0): 1, ('time', 1.0): 1, ('stay', 1.0): 1, ('away', 1.0): 1, ('varieti', 1.0): 1, ('pack', 1.0): 1, ('two', 1.0): 1, ('come', 1.0): 1, ('worth', 1.0): 1, ('money', 1.0): 1, ('buy', 0.0): 1, ('pet', 0.0): 1, ('store', 0.0): 1, ('web', 0.0): 1, ('rip', 0.0): 1, ('sorri', 0.0): 1}\n"
     ]
    }
   ],
   "source": [
    "# choose some random review\n",
    "review_1 = positive_reviews[101]\n",
    "review_2 = positive_reviews[10700]\n",
    "print(review_1, '\\n')\n",
    "print(process_review(review_1), '\\n')\n",
    "print(review_2, '\\n')\n",
    "print(process_review(review_2), '\\n')\n",
    "\n",
    "review_y = np.append(np.ones((1, 1)), np.zeros((1, 1)), axis = 0)\n",
    "\n",
    "review = [review_1, review_2]\n",
    "freqs = build_freqs(review, review_y) # Build frequencies\n",
    "\n",
    "print('frequencies:')\n",
    "print(freqs) # Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.  \n",
    "Logistic regression:\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$  \n",
    "  \n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$  \n",
    "\n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j $$\n",
    "\n",
    "To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "$$\\theta = \\theta - \\frac{\\alpha}{m} \\times (x^{T} \\cdot (h-y)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    h = 1/(1+(np.exp(-z)))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    # the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # calculate the cost function\n",
    "        J = -1./m*(np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m)*np.dot(x.T, (h-y))\n",
    "\n",
    "    return float(J), theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(review, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        review: a string of words for one review\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_review(review)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0), 0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-11, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.56264156.\n",
      "The resulting vector of weights is [0.0, 1.853e-05, -2.627e-05]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(review, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        review: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a review being positive or negative\n",
    "    '''\n",
    "    # extract the features of the review and store it into x\n",
    "    x = extract_features(review, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of reviews\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        y_hat: the predictions\n",
    "    \"\"\"\n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for review in test_x:\n",
    "        # get the label prediction for the review\n",
    "        y_pred = predict_review(review, freqs, theta)\n",
    "        y_hat.append(1.0) if y_pred > 0.5 else y_hat.append(0)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = test_logistic_regression(test_x, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(test_y, y_hat):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of reviews\n",
    "        y_hat: the predictions\n",
    "    Output:\n",
    "        accuracy: (# of reviews classified correctly) / (total # of reviews)\n",
    "        precision: TP/(TP+FP)\n",
    "        recall: TP/(TP+FN)\n",
    "        f1_score: 2*(precision*recall)/(precision+recall)\n",
    "    \"\"\"\n",
    "    test_y = pd.Series((i[0] for i in test_y), name='Actual')\n",
    "    y_hat = pd.Series(y_hat, name='Predicted')\n",
    "\n",
    "    confusion = pd.crosstab(test_y, y_hat) \n",
    "    tn = confusion[0][0]\n",
    "    fn = confusion[0][1]\n",
    "    fp = confusion[1][0]\n",
    "    tp = confusion[1][1]\n",
    "    \n",
    "    accuracy = (tn+tp)/(tn+tp+fp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.7325\n",
      "Logistic regression model's precision = 0.7649\n",
      "Logistic regression model's recall = 0.8374\n",
      "Logistic regression model's f1-score = 0.7995\n"
     ]
    }
   ],
   "source": [
    "scores = scoring(test_y, y_hat)\n",
    "print(f\"Logistic regression model's accuracy = {scores[0]:.4f}\")\n",
    "print(f\"Logistic regression model's precision = {scores[1]:.4f}\")\n",
    "print(f\"Logistic regression model's recall = {scores[2]:.4f}\")\n",
    "print(f\"Logistic regression model's f1-score = {scores[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_review(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE REVIEW IS:', x)\n",
    "        print('THE PROCESSED REVIEW IS:', process_review(x))\n",
    "        print('%d\\t%0.8f\\t' % (y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict my own review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'tast', 'meh']\n",
      "[[0.45068514]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "my_review = 'This food tasted meh'\n",
    "print(process_review(my_review))\n",
    "y_pred = predict_review(my_review, freqs, theta)\n",
    "print(y_pred)\n",
    "if y_pred > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
